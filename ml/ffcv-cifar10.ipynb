{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7cb9b5d-3b72-4ec7-b98f-0ccd85c7d13e",
   "metadata": {},
   "source": [
    "# Training CIFAR-10 using FFCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875bcacc-3f7b-4331-a868-e06276ec4ab5",
   "metadata": {},
   "source": [
    "Fast Forward Computer Vision ([FFCV](https://ffcv.io/)): train models with accelerated data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f953edbd-d8b7-4282-86b9-6742346396e9",
   "metadata": {},
   "source": [
    "GitHub: https://github.com/libffcv/ffcv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7cb6e5-0980-4e34-adef-26695859852a",
   "metadata": {},
   "source": [
    "Installation\n",
    "```\n",
    "conda create -y -n ffcv python=3.9 cupy pkg-config compilers libjpeg-turbo opencv pytorch torchvision cudatoolkit=11.3 numba -c pytorch -c conda-forge\n",
    "conda activate ffcv\n",
    "pip install ffcv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545d9e92-8f42-4075-b092-e13987bcb687",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b6e179-f175-466e-82ae-e7621e8ce23a",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3410c1d8-2c3a-4551-9b08-a32ff389fa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.cuda.amp as amp\n",
    "import torchvision\n",
    "\n",
    "import ffcv\n",
    "import ffcv.fields as fields\n",
    "import ffcv.fields.decoders as decoders\n",
    "import ffcv.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264ae849-b401-4a79-9d66-0a548765a332",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75737310-5da6-4356-b402-623f75660b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR='./data'\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "CIFAR_MEAN = [125.307, 122.961, 113.8575]\n",
    "CIFAR_STD = [51.5865, 50.847, 51.255]\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "NUM_WORKERS = 20\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "190473f9-4e67-4175-8622-108462d9d050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c08f8e8-cd0f-4b89-97b7-b64251d250f6",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d431cb-d8c3-436d-a639-8203f3a82f4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Convert datasets to FFCV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbaeb501-a498-412a-be29-45728d93fd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataset(dset, name):\n",
    "    writer = ffcv.writer.DatasetWriter(name + '.beton', {\n",
    "        'image': fields.RGBImageField(),\n",
    "        'label': fields.IntField()\n",
    "    })\n",
    "    writer.from_indexed_dataset(dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbbd5942-9eb4-4581-8d17-1e01b744d325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dset = torchvision.datasets.CIFAR10(DATA_DIR, train=True, download=True)\n",
    "test_dset = torchvision.datasets.CIFAR10(DATA_DIR, train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea59f8a3-eb71-4438-bc3f-84c2da389ebf",
   "metadata": {},
   "source": [
    "Convert to FFCV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bb071b5-e1d4-4f4a-91b3-64c819e77ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 50000/50000 [00:00<00:00, 82795.60it/s]\n",
      "100%|██████████████████████████████████| 10000/10000 [00:00<00:00, 24838.28it/s]\n"
     ]
    }
   ],
   "source": [
    "convert_dataset(train_dset, 'cifar_train')\n",
    "convert_dataset(test_dset, 'cifar_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13894ce8-efa6-49db-a4a3-1d199299044c",
   "metadata": {},
   "source": [
    "### Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9883f13e-4de2-4188-8dfc-42ff36e50d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_pipeline(train=True):\n",
    "    augmentation_pipeline = [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomTranslate(padding=2),\n",
    "        transforms.Cutout(8, tuple(map(int, CIFAR_MEAN)))\n",
    "    ] if train else []\n",
    "    \n",
    "    image_pipeline = [\n",
    "        decoders.SimpleRGBImageDecoder()\n",
    "    ] + augmentation_pipeline + [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.ToDevice(DEVICE, non_blocking=True),\n",
    "        transforms.ToTorchImage(),\n",
    "        transforms.Convert(torch.float16),\n",
    "        torchvision.transforms.Normalize(CIFAR_MEAN, CIFAR_STD)\n",
    "    ]\n",
    "    return image_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a69c09e0-7c02-4cae-b376-d47bd85ef21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pipeline = [\n",
    "    decoders.IntDecoder(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.ToDevice(DEVICE),\n",
    "    transforms.Squeeze()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1af15b32-52a8-4d23-9ab6-a570e25ba0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_pipeline = get_image_pipeline(train=True)\n",
    "test_image_pipeline = get_image_pipeline(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0bf142d-e9e4-408d-b63d-0eff6ac03046",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = ffcv.loader.Loader(f'cifar_train.beton',\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  num_workers=NUM_WORKERS,\n",
    "                                  order=ffcv.loader.OrderOption.RANDOM,\n",
    "                                  drop_last=True,\n",
    "                                  pipelines={'image': train_image_pipeline,\n",
    "                                             'label': label_pipeline})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0c5c5f5-d93b-45ab-86a6-8da96a896233",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = ffcv.loader.Loader(f'cifar_test.beton',\n",
    "                                 batch_size=BATCH_SIZE,\n",
    "                                 num_workers=NUM_WORKERS,\n",
    "                                 order=ffcv.loader.OrderOption.SEQUENTIAL,\n",
    "                                 drop_last=False,\n",
    "                                 pipelines={'image': test_image_pipeline,\n",
    "                                            'label': label_pipeline})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913753d5-dafa-4410-a8fd-ce648ee6d08c",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44e220de-2150-4722-859c-76b76eaf35b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, act=True):\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        ]\n",
    "        if act: layers.append(nn.SiLU(inplace=True))\n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf53d0ca-2239-4b5d-82ec-48cd69896e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.residual = nn.Sequential(\n",
    "            ConvBlock(in_channels, out_channels),\n",
    "            ConvBlock(out_channels, out_channels, act=False)\n",
    "        )\n",
    "        self.shortcut = self.get_shortcut(in_channels, out_channels)\n",
    "        self.act = nn.SiLU(inplace=True)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.shortcut(x) + self.gamma * self.residual(x)\n",
    "        return self.act(out)\n",
    "    \n",
    "    def get_shortcut(self, in_channels, out_channels):\n",
    "        if in_channels != out_channels:\n",
    "            shortcut = ConvBlock(in_channels, out_channels, 1, act=False)\n",
    "        else:\n",
    "            shortcut = nn.Identity()\n",
    "        return shortcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "269ac50a-01a3-4c14-b6ba-c034332a74ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualStack(nn.Sequential):\n",
    "    def __init__(self, in_channels, channels_list, num_blocks_list, strides):\n",
    "        layers = []\n",
    "        for num_blocks, out_channels, stride in zip(num_blocks_list, channels_list, strides):\n",
    "            if stride > 1: layers.append(nn.MaxPool2d(stride))\n",
    "            for _ in range(num_blocks):\n",
    "                layers.append(ResidualBlock(in_channels, out_channels))\n",
    "                in_channels = out_channels\n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55201525-918b-4d24-bd86-31e899cade7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stem(nn.Sequential):\n",
    "    def __init__(self, in_channels, channels_list, stride):\n",
    "        layers = []\n",
    "        for out_channels in channels_list:\n",
    "            layers.append(ConvBlock(in_channels, out_channels, 3, stride=stride))\n",
    "            in_channels = out_channels\n",
    "            stride = 1\n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "378b966e-2c12-49c5-bd4f-076b18dc6e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Sequential):\n",
    "    def __init__(self, in_channels, classes, p_drop=0.):\n",
    "        super().__init__(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(in_channels, classes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "386616de-2711-4f30-bba2-507254126bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Sequential):\n",
    "    def __init__(self, classes, num_blocks_list, channels_list, strides, in_channels=3, head_p_drop=0.):\n",
    "        super().__init__(\n",
    "            Stem(in_channels, [32, 32, 64], strides[0]),\n",
    "            ResidualStack(64, channels_list, num_blocks_list, strides[1:]),\n",
    "            Head(channels_list[-1], classes, head_p_drop)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4dcfed80-06c9-494d-84b0-4513812867df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_linear(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None: nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd6e61b0-f6da-4c77-b52f-fa7ba6f49356",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(NUM_CLASSES,\n",
    "               num_blocks_list=[2, 2, 2],\n",
    "               channels_list=[64, 128, 256],\n",
    "               strides=[1, 1, 2, 2],\n",
    "               head_p_drop=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80cd2e18-d4b1-4ad9-8fb7-983a98808f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.apply(init_linear);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "061b7dce-56dc-4b48-b9cc-4fbd805720af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(memory_format=torch.channels_last).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a52d849-b530-497f-9743-00ca6016103a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 2,804,592\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of parameters: {:,}\".format(sum(p.numel() for p in model.parameters())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ad23a0-7e38-4a1b-899c-2322eba35c9d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3c6104e-e258-4c05-a940-ed64c2758e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "edea3a24-06ce-4fd4-822c-fedc778a9c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a69ca603-163d-4d10-be1a-0d900f465197",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-2, steps_per_epoch=len(train_loader), epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02b4c84a-a6d5-43a3-9ed7-1240125ece2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4c7acb7-1879-4589-a9c0-72d35cc3fcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/50: train loss: 2.023; val accuracy: 0.379\n",
      "1/50: train loss: 1.616; val accuracy: 0.524\n",
      "2/50: train loss: 1.400; val accuracy: 0.583\n",
      "3/50: train loss: 1.255; val accuracy: 0.654\n",
      "4/50: train loss: 1.145; val accuracy: 0.664\n",
      "5/50: train loss: 1.073; val accuracy: 0.745\n",
      "6/50: train loss: 1.001; val accuracy: 0.669\n",
      "7/50: train loss: 0.952; val accuracy: 0.775\n",
      "8/50: train loss: 0.908; val accuracy: 0.795\n",
      "9/50: train loss: 0.872; val accuracy: 0.802\n",
      "10/50: train loss: 0.839; val accuracy: 0.825\n",
      "11/50: train loss: 0.815; val accuracy: 0.787\n",
      "12/50: train loss: 0.786; val accuracy: 0.808\n",
      "13/50: train loss: 0.764; val accuracy: 0.850\n",
      "14/50: train loss: 0.745; val accuracy: 0.873\n",
      "15/50: train loss: 0.722; val accuracy: 0.869\n",
      "16/50: train loss: 0.704; val accuracy: 0.876\n",
      "17/50: train loss: 0.687; val accuracy: 0.873\n",
      "18/50: train loss: 0.678; val accuracy: 0.868\n",
      "19/50: train loss: 0.664; val accuracy: 0.884\n",
      "20/50: train loss: 0.650; val accuracy: 0.889\n",
      "21/50: train loss: 0.640; val accuracy: 0.889\n",
      "22/50: train loss: 0.630; val accuracy: 0.883\n",
      "23/50: train loss: 0.616; val accuracy: 0.897\n",
      "24/50: train loss: 0.613; val accuracy: 0.896\n",
      "25/50: train loss: 0.601; val accuracy: 0.904\n",
      "26/50: train loss: 0.591; val accuracy: 0.906\n",
      "27/50: train loss: 0.582; val accuracy: 0.911\n",
      "28/50: train loss: 0.576; val accuracy: 0.906\n",
      "29/50: train loss: 0.569; val accuracy: 0.915\n",
      "30/50: train loss: 0.563; val accuracy: 0.918\n",
      "31/50: train loss: 0.559; val accuracy: 0.915\n",
      "32/50: train loss: 0.554; val accuracy: 0.920\n",
      "33/50: train loss: 0.549; val accuracy: 0.923\n",
      "34/50: train loss: 0.543; val accuracy: 0.920\n",
      "35/50: train loss: 0.538; val accuracy: 0.926\n",
      "36/50: train loss: 0.536; val accuracy: 0.930\n",
      "37/50: train loss: 0.533; val accuracy: 0.927\n",
      "38/50: train loss: 0.529; val accuracy: 0.928\n",
      "39/50: train loss: 0.527; val accuracy: 0.930\n",
      "40/50: train loss: 0.526; val accuracy: 0.931\n",
      "41/50: train loss: 0.524; val accuracy: 0.932\n",
      "42/50: train loss: 0.522; val accuracy: 0.932\n",
      "43/50: train loss: 0.521; val accuracy: 0.933\n",
      "44/50: train loss: 0.521; val accuracy: 0.933\n",
      "45/50: train loss: 0.520; val accuracy: 0.934\n",
      "46/50: train loss: 0.519; val accuracy: 0.934\n",
      "47/50: train loss: 0.519; val accuracy: 0.933\n",
      "48/50: train loss: 0.518; val accuracy: 0.934\n",
      "49/50: train loss: 0.518; val accuracy: 0.933\n"
     ]
    }
   ],
   "source": [
    "for e in range(EPOCHS):\n",
    "    print(f'{e}/{EPOCHS}: ', end='')\n",
    "    \n",
    "    model.train()\n",
    "    total_loss, total_num = 0., 0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with amp.autocast():\n",
    "            out = model(images)\n",
    "            loss = loss_fn(out, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        \n",
    "        batch_size = images.shape[0]\n",
    "        total_loss += batch_size * loss.item()\n",
    "        total_num += batch_size\n",
    "    \n",
    "    print(f'train loss: {total_loss / total_num:.3f}; ', end='')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_correct, total_num = 0., 0.\n",
    "        for images, labels in test_loader:\n",
    "            with amp.autocast():\n",
    "                out = (model(images) + model(torch.fliplr(images))) / 2. # Test-time augmentation\n",
    "                total_correct += (out.argmax(1) == labels).sum().cpu().item()\n",
    "                total_num += images.shape[0]\n",
    "\n",
    "        print(f'val accuracy: {total_correct / total_num:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ffcv]",
   "language": "python",
   "name": "conda-env-ffcv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
